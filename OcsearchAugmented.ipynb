{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy.interpolate import LinearNDInterpolator, RegularGridInterpolator\n",
    "from scipy.io import netcdf\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "earthRadiusKilometers = 6378.16\n",
    "COL_BEARING = 'bearing'\n",
    "COL_DATE = 'tagDate'\n",
    "COL_DEPTH = 'depth'\n",
    "COL_DEPTH_CHANGE_DOWNSTREAM = 'depthChangeDownstream'\n",
    "COL_DEPTH_CHANGE_UPSTREAM = 'depthChangeUpstream'\n",
    "COL_DISTANCESTEP = 'distanceStep'\n",
    "COL_LATITUDE = 'latitude'\n",
    "COL_LONGITUDE = 'longitude'\n",
    "COL_SHARK_ID = 'shark_id'\n",
    "COL_TIMESTEP = 'timeStep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendDepthAndDepthChanges(df, interpBathysphere, interpCurrentU, interpCurrentV):\n",
    "    # Create arrays with depth and downstream/upstream depth changes for each lon/lat coordinate\n",
    "    df[COL_DEPTH] = pd.Series (0., index=df.index)\n",
    "    df[COL_DEPTH_CHANGE_DOWNSTREAM] = pd.Series (0., index=df.index)\n",
    "    df[COL_DEPTH_CHANGE_UPSTREAM] = pd.Series (0., index=df.index)\n",
    "    lonLast = {}  # Indexed by shark id\n",
    "    indexTo = 0\n",
    "    for idRow, row in df.iterrows():\n",
    "        lon = row[COL_LONGITUDE]\n",
    "        lat = row[COL_LATITUDE]\n",
    "        print (\"whatever {} {}\" . format (lon, lat))\n",
    "\n",
    "        # Perform interpolations\n",
    "        depth = interpBathysphere([lat, lon])[0]\n",
    "        u = interpCurrentU([lon, lat])[0]\n",
    "        v = interpCurrentV([lon, lat])[0]\n",
    "\n",
    "        # Get downstream and upstream points\n",
    "        lonDownstream, latDownstream = separatedPointsFromSeparation(lon, lat, u, v)\n",
    "        lonUpstream, latUpstream = separatedPointsFromSeparation(lon, lat, -1.0 * u, -1.0 * v)\n",
    "\n",
    "        # More interpolations at downstream and upstream points\n",
    "        depthDownstream = interpBathysphere([latDownstream, lonDownstream])[0]\n",
    "        depthUpstream = interpBathysphere([latUpstream, lonUpstream])[0]\n",
    "\n",
    "        # Save results\n",
    "        idx = df.index[indexTo]\n",
    "        df.at [idx, COL_DEPTH] = depth\n",
    "        df.at [idx, COL_DEPTH_CHANGE_DOWNSTREAM] = depthDownstream - depth\n",
    "        df.at [idx, COL_DEPTH_CHANGE_UPSTREAM] = depth - depthUpstream\n",
    "\n",
    "        indexTo += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def appendDirectionAndLocationQuantities (df, interpDeclination):\n",
    "    # Create arrays using points I-1 and I:\n",
    "    # 1) absolute bearing angle (degrees), 0=magnetic north and +90=eastward\n",
    "    # 2) time between successive locations (hours)\n",
    "    # 3) distance between successive locations (kilometers)\n",
    "    # The last two quantities may be useful to account for how readings just a short time apart\n",
    "    # (minutes) may be highly correlated, but readings far apart in time (months) will be lacking\n",
    "    # much important information so maybe the correlations are less reliable\n",
    "    df[COL_BEARING] = pd.Series (0., index=df.index)\n",
    "    df[COL_TIMESTEP] = pd.Series (timedelta(0), index=df.index)\n",
    "    df[COL_DISTANCESTEP] = pd.Series (0., index=df.index)\n",
    "    indexTo = 0\n",
    "    indexBearing = df.columns.get_loc(COL_BEARING)\n",
    "    indexTimeStep = df.columns.get_loc(COL_TIMESTEP)\n",
    "    indexDistanceStep = df.columns.get_loc(COL_DISTANCESTEP)\n",
    "    rowLast = {} # Indexed by shark id\n",
    "    for idRow, row in df.iterrows():\n",
    "        idShark = row [COL_SHARK_ID]\n",
    "        lon = row [COL_LONGITUDE]\n",
    "        lat = row [COL_LATITUDE]\n",
    "        time = row [COL_DATE]\n",
    "\n",
    "        # Perform calculations\n",
    "        bearing = 0.\n",
    "        timeStep = time - time\n",
    "        distanceStep = 0.\n",
    "        if idShark in rowLast:\n",
    "            lonLast = rowLast [idShark] [COL_LONGITUDE]\n",
    "            latLast = rowLast [idShark] [COL_LATITUDE]\n",
    "            timeLast = rowLast [idShark] [COL_DATE]\n",
    "\n",
    "            # This code assumes duplicate id/timestamp rows have been removed\n",
    "            bearing = bearingFromSeparatedPoints (interpDeclination, lonLast, latLast, lon, lat)\n",
    "            timeStep = time - timeLast\n",
    "            distanceStep = separationFromSeparatedPoints (lonLast, latLast, lon, lat)\n",
    "        idx = df.index[indexTo]\n",
    "        df.at [idx, COL_BEARING] = bearing\n",
    "        df.at [idx, COL_TIMESTEP] = timeStep\n",
    "        df.at [idx, COL_DISTANCESTEP] = distanceStep\n",
    "        indexTo += 1\n",
    "        rowLast [idShark] = row\n",
    "\n",
    "    return df\n",
    "\n",
    "def bearingFromSeparatedPoints (interpDeclination, lon0, lat0, lon1, lat1):\n",
    "    # Inverse of separatedPointsFromSeparation.\n",
    "    # For small enough separations, we can ignore the distortion caused by the\n",
    "    # longitude lines joining at the north pole, and just convert angular separation into distance\n",
    "    angleDeclination = interpDeclination ([lon0, lat0])\n",
    "    # Angle from north pole, ignoring magnetic declination. Note that angle measured from\n",
    "    # eastward direction would be atan2 (lat1 - lat0, lon1 - lon0)\n",
    "    angleTrueNorth = 180. * math.atan2 (lon1 - lon0, lat1 - lat0) / np.pi\n",
    "    angleMagneticNorth = angleTrueNorth - angleDeclination\n",
    "    return angleMagneticNorth\n",
    "\n",
    "def check (interp, title):\n",
    "    lonmin = -80\n",
    "    lonmax = -35\n",
    "    latmin = 10\n",
    "    latmax = 45\n",
    "    lons = np.linspace (lonmin + 1, lonmax - 1, 240)\n",
    "    lats = np.linspace (latmin + 1, latmax - 1, 240)\n",
    "    lons, lats = np.meshgrid (lons, lats)\n",
    "    lonLat =  np.stack ((lons, lats), axis = -1)\n",
    "    values = interp (lonLat)\n",
    "    plt.title (title)\n",
    "    plt.pcolor (lons, lats, values)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def loadBathysphereAtlantic ():\n",
    "    print (\"loadBathysphereAtlantic\")\n",
    "\n",
    "    # This function is much faster than loadBathysphereWorld because it covers a small area\n",
    "    # etopo1_bedrock_-80_-35_10_45.nc\n",
    "    units = 'meters'    \n",
    "    googleIdBathysphere = '10VqbV2oNUVcvS6lLP3FekVlFM4LUJj5o' # Extracted from share url\n",
    "    tmpBathysphere = tempfile.NamedTemporaryFile (suffix = '.nc', \\\n",
    "                                                  prefix = 'tempBathysphere', \\\n",
    "                                                  delete = True) # Need file name but not the file or gdd fails\n",
    "    tmpBathysphere.close()\n",
    "    # Download the file from url and save it locally\n",
    "    gdd.download_file_from_google_drive (file_id = googleIdBathysphere,\n",
    "                                         dest_path = tmpBathysphere.name)\n",
    "    with netcdf.netcdf_file (tmpBathysphere.name, 'r', mmap = False) as f:\n",
    "        loncdf = f.variables ['lon'] # 1D data going from lonmin to lonmax\n",
    "        latcdf = f.variables ['lat'] # 1D data going from latmin to latmax\n",
    "        elecdf = f.variables ['Band1'] # 2D data indexed by (lat,lon)       \n",
    "        crscdf = f.variables ['crs'] # Do not know what this array contains, other than 1 character strings\n",
    "\n",
    "    # Create an interpolator. This is a regular grid so we use a regular grid interpolator that\n",
    "    # exploits the regularity to achieve the most efficient search\n",
    "    return units, RegularGridInterpolator ((latcdf.data, loncdf.data), elecdf.data)\n",
    "\n",
    "def loadBathysphereWorld ():\n",
    "    print (\"loadBathysphereWorld\")\n",
    "\n",
    "    # etopo1_bedrock_-M_-30_-N_50.nc\n",
    "    # where (M,N) = (-180,-120) (-120,-60) (-60,0) (0,60) (60,120) (120,180)\n",
    "    units = 'meters'    \n",
    "    # Extracted from share url\n",
    "    googleIdBathyspheres = [\n",
    "        '1bxT1MuGjpa-gGA-45NQA3hmTrblr_h7R',\n",
    "        '1r8blFCsLdEvWOyZ80fRNxq_pIYYbAM5m',\n",
    "        '1eMj03kwp3biK1HCzJKI3TvkA62xihMJm',\n",
    "        '1rvq8mrm58RQzPZWA_d2vrzxgqeJ28ImO',\n",
    "        '1GUWrfQ0FBuBGqRhd389tcJ_0_82smhDG',\n",
    "        '1Fc3xEF4gs0xDVCUdZrNuRVDJx1huXkMf'\n",
    "    ]\n",
    "    lons = None\n",
    "    lats = None\n",
    "    eles = None\n",
    "    for googleIdBathysphere in googleIdBathyspheres:\n",
    "        tmpBathysphere = tempfile.NamedTemporaryFile (suffix = '.nc', \\\n",
    "                                                      prefix = 'tempBathysphere', \\\n",
    "                                                      delete = True) # Need file name but not the file or gdd fails\n",
    "        tmpBathysphere.close()\n",
    "        # Download the file from url and save it locally\n",
    "        gdd.download_file_from_google_drive (file_id = googleIdBathysphere,\n",
    "                                             dest_path = tmpBathysphere.name)\n",
    "        with netcdf.netcdf_file (tmpBathysphere.name, 'r', mmap = False) as f:\n",
    "            loncdf = f.variables ['lon'] # 1D data going from lonmin to lonmax\n",
    "            latcdf = f.variables ['lat'] # 1D data going from latmin to latmax\n",
    "            elecdf = f.variables ['Band1'] # 2D data indexed by (lat,lon)\n",
    "            crscdf = f.variables ['crs'] # Do not know what this array contains, other than 1 character strings\n",
    "            \n",
    "        # Convert 1D longitude and latitude arrays to 2D since latitude arrays change between chunks\n",
    "        # so a single 1D array of latitudes across all chunks would not work\n",
    "        nlon = loncdf.data.shape[0]\n",
    "        nlat = latcdf.data.shape[0]\n",
    "        # Aggregate with longitudes changing but latitudes repeating\n",
    "        if lons is None:\n",
    "            lons = loncdf.data\n",
    "            lats = latcdf.data\n",
    "            eles = elecdf.data\n",
    "        else:\n",
    "            # First column(s) of new columns may overlap last column(s) of previous columns which triggers \n",
    "            # an error so we delete the last column(s) of previous columns\n",
    "            nlons = len (lons)\n",
    "            while (lons [nlons - 1] >= loncdf.data [0]):\n",
    "                lons = np.delete (lons, nlons - 1, 0)\n",
    "                eles = np.delete (eles, nlons - 1, 1)\n",
    "                nlons -= 1\n",
    "            # The actual aggregation\n",
    "            lons = np.concatenate ((lons, loncdf.data), axis=0)\n",
    "            eles = np.concatenate ((eles, elecdf.data), axis=1)\n",
    "        print ('aggregate=({} {} {})' . format (lons.shape, lats.shape, eles.shape))\n",
    "    # Create an interpolator\n",
    "    np.savetxt('lons.csv', lons)\n",
    "    np.savetxt('lats.csv', lats)\n",
    "    return units, RegularGridInterpolator ((lats, lons), eles)\n",
    "\n",
    "def loadCurrent():\n",
    "    print (\"loadCurrent\")\n",
    "\n",
    "    # https://data.nodc.noaa.gov/thredds/ncss/ncep/rtofs/2017/201703/ofs.20170321/surface/ofs_atl.t00z.n000.20170321.grb.grib2/dataset.html\n",
    "    googleIdCurrent = '1ZL2ABGc5uqtBt9DK0_m7CxJPMBgpDrW3' # Extracted from share url\n",
    "    \n",
    "    tmpCurrent = tempfile.NamedTemporaryFile (suffix = '.nc', \\\n",
    "                                              prefix = 'tempCurrent', \\\n",
    "                                              delete = True) # Need file name but not the file or gdd fails\n",
    "    tmpCurrent.close()\n",
    "    # Download the file from url and save it locally\n",
    "    gdd.download_file_from_google_drive (file_id = googleIdCurrent, \\\n",
    "                                         dest_path = tmpCurrent.name)\n",
    "    with netcdf.netcdf_file (tmpCurrent.name, 'r', mmap = False) as f:\n",
    "        loncdf = f.variables ['Longitude_of_Presure_Point_surface']\n",
    "        latcdf = f.variables ['Latitude_of_Presure_Point_surface']        \n",
    "        ucdf = f.variables ['Barotropic_U_velocity_entire_ocean_single_layer'] # 1x1684x1200\n",
    "        vcdf = f.variables ['Barotropic_V_velocity_entire_ocean_single_layer'] # 1x1684x1200\n",
    "        units = 'm.s-1'\n",
    "\n",
    "    # Create interpolators. This is an irregular grid (not constant longitude and latitude points)\n",
    "    # so an inefficient irregular grid is applied\n",
    "    nx = loncdf.data.shape[0]\n",
    "    ny = latcdf.data.shape[1]\n",
    "    lonlat = []\n",
    "    u = []\n",
    "    v = []\n",
    "    for i in range (nx):\n",
    "        for j in range (ny):\n",
    "            lonlat.append ([loncdf[i][j], latcdf[i][j]])\n",
    "            u.append (ucdf[0][i][j])\n",
    "            v.append (vcdf[0][i][j])\n",
    "    return units, \\\n",
    "        LinearNDInterpolator (lonlat, u), \\\n",
    "        LinearNDInterpolator (lonlat, v)\n",
    "\n",
    "def loadDeclination ():\n",
    "    print (\"loadDeclination\")\n",
    "\n",
    "    # https://maps.ngdc.noaa.gov/viewers/historical_declination/\n",
    "    units = 'Degrees'        \n",
    "    googleIdDeclination = '1KL-brszjyfiX7yAp_-ZEBbereOm-26lz' # Extracted from share url    \n",
    "    tmpDeclination = tempfile.NamedTemporaryFile (suffix = '.nc', \\\n",
    "                                                  prefix = 'tempDeclination', \\\n",
    "                                                  delete = True) # Need file name but not the file or gdd fails\n",
    "    tmpDeclination.close()\n",
    "    # Download the file from url and save it locally\n",
    "    gdd.download_file_from_google_drive (file_id = googleIdDeclination,\n",
    "                                         dest_path = tmpDeclination.name)\n",
    "    with netcdf.netcdf_file (tmpDeclination.name, 'r', mmap = False) as f:\n",
    "        loncdf = f.variables ['x']\n",
    "        latcdf = f.variables ['y']\n",
    "        deccdf = f.variables ['z']\n",
    "\n",
    "    # Transpose lat/lon to lon/lat\n",
    "    dec = np.transpose (deccdf.data)\n",
    "    \n",
    "    # Create an interpolator. This is a regular grid so we use a regular grid interpolator that\n",
    "    # exploits the regularity to achieve the most efficient search\n",
    "    return units, RegularGridInterpolator ((loncdf.data, latcdf.data), dec)\n",
    "\n",
    "def loadSharkPathGallagher():\n",
    "    print (\"loadSharkPathGallagher\")    \n",
    "    # Processing code courtesy of Smitesh\n",
    "    # Upload the CSV Here\n",
    "    # from google.colab import files\n",
    "    # uploaded = files.upload()\n",
    "\n",
    "    # # Replace the filename here if you have saved the CSV as a different\n",
    "    # df = pd.read_csv(io.BytesIO(uploaded[\n",
    "    #     'Beneath The Waves - Blue Shark Atlantic - Data Jan 21, 2019.csv'])) \n",
    "\n",
    "    googleFile = 'https://drive.google.com/uc?id=1XtdF630BEDDv-ixbZ6cE4RJlbVwukiUU&export=download'\n",
    "    print ('Downloading {}... ' . format (googleFile), end='')    \n",
    "    df = pd.read_csv(googleFile)\n",
    "    print ('Done.')\n",
    "\n",
    "    # Next step is to clean the Data and drop the columns we don't need\n",
    "    COLUMN_MAPPING = {\n",
    "        'Shark ID': COL_SHARK_ID,\n",
    "        'Prg No.': 'prg_no',\n",
    "        'Latitude': COL_LATITUDE,\n",
    "        'Longitude': COL_LONGITUDE,\n",
    "        'Loc. quality': 'loc_quality',\n",
    "        'Loc. date': COL_DATE,\n",
    "        'Loc. type': 'loc_type',\n",
    "        'Altitude': 'alt',\n",
    "        'Pass': 'pass',\n",
    "        'Sat.': 'satellite',\n",
    "        'Frequency': 'freq',\n",
    "        'Msg Date': 'msg_date',\n",
    "        'Comp.': 'comp',\n",
    "        'Msg': 'msg',\n",
    "        '> - 120 DB': 'db_120_gt',\n",
    "        'Best level': 'best_level',\n",
    "        'Delta freq.': 'delta_freq',\n",
    "        'Long. 1': 'long_1', \n",
    "        'Lat. sol. 1': 'late_sol_1', \n",
    "        'Long. 2': 'long_2',\n",
    "        'Lat. sol. 2': 'lat_sol_2', \n",
    "        'Loc. idx': 'loc_idx', \n",
    "        'Nopc': 'nopc', \n",
    "        'Error radius': 'err_radius', \n",
    "        'Semi-major axis': 'semi_major_axis',\n",
    "        'Semi-minor axis': 'semi_minor_axis', \n",
    "        'Ellipse orientation': 'ellipse_orientation', \n",
    "        'GDOP': 'gdop'\n",
    "      }\n",
    "\n",
    "    # Drop Columns with no location data\n",
    "    cleaned_df = df.dropna(subset=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Drop Columns with bad location data quality\n",
    "    cleaned_df = cleaned_df.loc[cleaned_df['Loc. quality'].apply(str.isdigit)]\n",
    "    \n",
    "    # Select the important columns\n",
    "    cleaned_df = cleaned_df[list(COLUMN_MAPPING.keys())]\n",
    "    \n",
    "    # Rename the columns to be more pythonic\n",
    "    cleaned_df = cleaned_df.rename(columns=COLUMN_MAPPING)\n",
    "\n",
    "    # Cast to datetime values to datetime\n",
    "    cleaned_df[COL_DATE] = cleaned_df.loc_date.apply(lambda x: datetime.strptime(x, '%m/%d/%y %H:%M'))\n",
    "\n",
    "    # Save to csv for more detailed inspection\n",
    "    cleaned_df.to_csv ('outputs/cleaned_df_duplicates_included.csv')\n",
    "    \n",
    "    # Remove successive entries that are so close in time that the longitude\n",
    "    # and latitude coordinates are unchanged. This is experimental\n",
    "    cleaned_df = cleaned_df.drop_duplicates (subset = [COL_SHARK_ID, COL_LONGITUDE, COL_LATITUDE])\n",
    "\n",
    "    # Save to csv for more detailed inspection\n",
    "    cleaned_df.to_csv ('outputs/cleaned_df_duplicates_removed.csv')\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "def loadSharkPathOcsearch():\n",
    "    \n",
    "    # Processing code courtesy of \n",
    "    # https://github.com/botwranglers/ocearch/blob/master/solutions/Query%20Ocearch%20API.ipynb\n",
    "    url = 'http://www.ocearch.org/tracker/ajax/filter-sharks'\n",
    "    headers = {'Accept' : 'application/json'}\n",
    "    \n",
    "    # Download\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    df = pd.DataFrame (resp.json())\n",
    "    \n",
    "    # Extract just the pings so we eventually simplify the whole data structure\n",
    "    pingFrames=[]\n",
    "    for row in df.itertuples():\n",
    "        pingFrame = pd.DataFrame(row.pings)\n",
    "        pingFrame['id']=row.id\n",
    "        pingFrames.append(pingFrame)\n",
    "    len (pingFrames)\n",
    "    pings = pd.concat(pingFrames)\n",
    "\n",
    "    # Convert timestamp from string to datetime object\n",
    "    pings ['datetime'] = pd.to_datetime (pings.tz_datetime)\n",
    "\n",
    "    # tz_datetime duplicates datetime so remove it\n",
    "    pings.drop(columns=['tz_datetime'], inplace=True)\n",
    "    \n",
    "    # Columns from download that we want to keep\n",
    "    COLUMN_MAPPING = ['id',\n",
    "                      'name',\n",
    "                      'gender', \n",
    "                      'species', \n",
    "                      'weight',\n",
    "                      'length',\n",
    "                      COL_DATE,\n",
    "                      'dist_total']\n",
    "\n",
    "    # Merge the processed ping data. The ping data adds COL_LONGITUDE and COL_LATITUDE\n",
    "    joined = pings.merge (df [COLUMN_MAPPING], on='id')\n",
    "\n",
    "    return joined\n",
    "\n",
    "def main():\n",
    "    unitsBathysphere, interpBathysphere = loadBathysphereAtlantic()\n",
    "    unitsBathysphere, interpBathysphere = loadBathysphereWorld()    \n",
    "    #check (interpBathysphere, 'Bathysphere ({})' . format (unitsBathysphere))\n",
    "    \n",
    "    unitsCurrent, interpCurrentU, interpCurrentV = loadCurrent()\n",
    "    #check (interpCurrentU, 'CurrentU ({})' . format (unitsCurrent))\n",
    "    #check (interpCurrentV, 'CurrentV ({})' . format (unitsCurrent))\n",
    "    \n",
    "    unitsDeclination, interpDeclination = loadDeclination()\n",
    "    #check (interpDeclination, 'Declination ({})' . format (unitsDeclination))\n",
    "    \n",
    "    df = loadSharkPathOcsearch()\n",
    "    \n",
    "    df = appendDepthAndDepthChanges (df, interpBathysphere, interpCurrentU, interpCurrentV)\n",
    "    df = appendDirectionAndLocationQuantities (df, interpDeclination)\n",
    "\n",
    "    outfile = 'outputs/complete_df.csv'\n",
    "    print ('Writing csv file {}' . format (outfile))\n",
    "    df.to_csv (outfile)\n",
    "    \n",
    "def separatedPointsFromSeparation (lon, lat, u, v):\n",
    "    # Inverse of separationFromSeparatedPoints.\n",
    "    # Google Map investigation of Greater Bank bathysphere data suggests the 2 points used\n",
    "    # upstream and downstream (in terms of the current) should be about 10 miles from the\n",
    "    # center point\n",
    "    separationKilometers = 10.0 * (1.6 / 1.0)\n",
    "\n",
    "    # Make u and v into a unit vector (u,v) which will be multiplied by angleSeparation below\n",
    "    # to get a (lon,lat) separation vector with a specified great circle angle\n",
    "    uvmag = math.sqrt (u * u + v * v)\n",
    "    u = u / uvmag\n",
    "    v = v / uvmag\n",
    "    \n",
    "    # For small enough separationKilometers, we can ignore the distortion caused by the\n",
    "    # longitude lines joining at the north pole, and just add longitude and latitude\n",
    "    # deltas calculated as simply proportional to u and v\n",
    "    angleSeparation = separationKilometers / earthRadiusKilometers # Great circle angle in radians\n",
    "    lonNew = lon + angleSeparation * u * 180. / np.pi\n",
    "    latNew = lat + angleSeparation * v * 180. / np.pi\n",
    "    return lonNew, latNew\n",
    "\n",
    "def separationFromSeparatedPoints (lon0, lat0, lon1, lat1):\n",
    "    # Inverse of separatedPointsFromSeparation. Returns great circle angle between two vectors.\n",
    "    # For small enough separations, we can ignore the distortion caused by the\n",
    "    # longitude lines joining at the north pole, and just convert angular separation into distance\n",
    "    angleSep = math.sqrt ((lon1 - lon0) * (lon1 - lon0) + \\\n",
    "                          (lat1 - lat0) * (lat1 - lat0))\n",
    "    return (angleSep * np.pi / 180.) * earthRadiusKilometers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadBathysphereAtlantic\n",
      "Downloading 10VqbV2oNUVcvS6lLP3FekVlFM4LUJj5o into /tmp/tempBathyspheree0d61aol.nc... Done.\n",
      "loadBathysphereWorld\n",
      "Downloading 1bxT1MuGjpa-gGA-45NQA3hmTrblr_h7R into /tmp/tempBathysphereoklkktiw.nc... Done.\n",
      "aggregate=((3601,) (5401,) (5401, 3601))\n",
      "Downloading 1r8blFCsLdEvWOyZ80fRNxq_pIYYbAM5m into /tmp/tempBathysphereqfh13_r5.nc... Done.\n",
      "aggregate=((7202,) (5401,) (5401, 7202))\n",
      "Downloading 1eMj03kwp3biK1HCzJKI3TvkA62xihMJm into /tmp/tempBathysphere4fa4_ibs.nc... Done.\n",
      "aggregate=((10802,) (5401,) (5401, 10802))\n",
      "Downloading 1rvq8mrm58RQzPZWA_d2vrzxgqeJ28ImO into /tmp/tempBathyspherevynto2i4.nc... Done.\n",
      "aggregate=((14403,) (5401,) (5401, 14403))\n",
      "Downloading 1GUWrfQ0FBuBGqRhd389tcJ_0_82smhDG into /tmp/tempBathysphereyo1gn1ux.nc... Done.\n",
      "aggregate=((18003,) (5401,) (5401, 18003))\n",
      "Downloading 1Fc3xEF4gs0xDVCUdZrNuRVDJx1huXkMf into /tmp/tempBathyspheremlkz5h4b.nc... Done.\n",
      "aggregate=((21604,) (5401,) (5401, 21604))\n",
      "loadCurrent\n",
      "Downloading 1ZL2ABGc5uqtBt9DK0_m7CxJPMBgpDrW3 into /tmp/tempCurrentpt6385pr.nc... Done.\n",
      "loadDeclination\n",
      "Downloading 1KL-brszjyfiX7yAp_-ZEBbereOm-26lz into /tmp/tempDeclination57_erz2z.nc... Done.\n",
      "whatever 21.15244 -34.60661\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'numpy.ndarray' and 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-a06b0e12d383>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadSharkPathOcsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappendDepthAndDepthChanges\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpBathysphere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpCurrentU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpCurrentV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappendDirectionAndLocationQuantities\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpDeclination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-a06b0e12d383>\u001b[0m in \u001b[0;36mappendDepthAndDepthChanges\u001b[0;34m(df, interpBathysphere, interpCurrentU, interpCurrentV)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Perform interpolations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpBathysphere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpCurrentU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpCurrentV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, xi, method)\u001b[0m\n\u001b[1;32m   2411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2413\u001b[0;31m                 if not np.logical_and(np.all(self.grid[i][0] <= p),\n\u001b[0m\u001b[1;32m   2414\u001b[0m                                       np.all(p <= self.grid[i][-1])):\n\u001b[1;32m   2415\u001b[0m                     raise ValueError(\"One of the requested xi is out of bounds \"\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'numpy.ndarray' and 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
